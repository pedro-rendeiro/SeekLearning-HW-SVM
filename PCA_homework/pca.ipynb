{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Homework\n",
    "### Micael Fernandes - 201906840029"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\t\n",
    "Quais as duas componentes principais? Quais os autovalores e as variâncias associadas com estas duas componentes principais?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dados:\n",
    "X=\n",
    "[ 1 0\n",
    "  2 0\n",
    "  3 0\n",
    "  5 6\n",
    "  6 6\n",
    "  7 6 ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img alt=\"geo1\" src=\"./imgs/geo1.png\">\n",
    "<img alt=\"geo5\"src=\"./imgs/geo5.jpg\">\n",
    "<img alt=\"geo2\" src=\"./imgs/geo2.png\">\n",
    "<img alt=\"geo3\" src=\"./imgs/geo3.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\t\n",
    "Considere que realizamos uma PCA em um conjunto de dados bidimensional e obtemos os autovalores 6 e 2. Desenhe uma distribuição de pontos amostrais que podem dar origem a este resultado. Além disso, desenhe os dois autovetores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"sol1\" src=\"./imgs/sol1.jpg\">\n",
    "<img alt=\"geo6\" src=\"./imgs/geo6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)\t\n",
    "Considere 3 pontos de dados em um espaço bidimensional R**2: (-1, 1), (0, 0), (1, 1). Qual é o primeiro componente principal do conjunto de dados fornecido?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"geo7\" src=\"./imgs/geo7.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "Autovetores: \n",
      " [[-0.70710678  0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n",
      "Autovalores: \n",
      " [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.array([[-1, 1], [0, 0], [1, 1]])\n",
    "my_pca = PCA(2)\n",
    "my_pca.fit_transform(d)\n",
    "print(\"Autovetores: \\n\", my_pca.eigenvectors)\n",
    "print(\"Autovalores: \\n\",my_pca.eigenvalues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)\t\n",
    "Suponha que realizamos PCA em um conjunto de dados bidimensional e o resultado são dois autovalores iguais. O que isso significa em relação à importância de cada dimensão? Buscar uma redução de dimensionalidade seria uma boa escolha neste caso? Por favor explique. Esboce um conjunto de dados em que seus dois autovalores tenham o mesmo tamanho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Solução: </b></br>\n",
    "Os valores teram igual variancia de componentes principais. Assim não tera muita variabilidade e a \"importancia\" dos dados para aquele treinamento será a mesma. Ou seja, cada dimensão terá a mesma importacia e impactara no aprendizado. Logo, buscando a maxima exploração dos dados para o modelo não perder informação, não é muito viavel a reduzir a dimensionalidade nesse caso, uma vez que a variabilidade dos PCAs é a mesma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)\t\n",
    "Explique o motivo de uso de SVD na PCA do scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"svd\" src=\"./imgs/svd.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)\n",
    "Acompanhe o tutorial em [2] e implemente o código em Python que realiza a PCA em um dataset com mais de 3 parâmetros de entrada (“features”), de forma a converter este dataset em um novo dataset com apenas duas “features”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvectors: \n",
      " [[-6.99241769e-01  1.48734314e-01]\n",
      " [-1.05171042e-01 -9.88877194e-01]\n",
      " [-7.07106781e-01  1.35632157e-18]]\n",
      "Eigenvalues: \n",
      " [0.72658582 1.27341418 1.        ]\n",
      "PCA 1:  42.44713943900227\n",
      "PCA 2:  33.33333333333333\n",
      "PCA 3:  24.219527227664408\n",
      "Final Matrix: \n",
      " [[ 0.56037183 -1.35985659]\n",
      " [ 1.69540383 -0.14873431]\n",
      " [ 0.4154975   1.06238796]\n",
      " [-1.91677317  0.14873431]\n",
      " [-0.77773825 -1.06238796]\n",
      " [ 0.02323826  1.35985659]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PCA:\n",
    "    def __init__(self, n_components=2) -> None:\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        # get number of samples and components\n",
    "        self.n_samples = X.shape[0]\n",
    "\n",
    "        # standardize data\n",
    "        self.A = self.standardize_data(X)\n",
    "        # calculate covariance matrix\n",
    "        self.covariance_matrix = self.get_covariance_matrix()\n",
    "        # retrieve selected eigenvectors\n",
    "        self.eigenvectors = self.get_eigenvectors(self.covariance_matrix)\n",
    "        # project into lower dimension\n",
    "        self.projected_matrix = self.project_matrix(self.eigenvectors)\n",
    "        return self.projected_matrix\n",
    "\n",
    "    def standardize_data(self, X):\n",
    "        # subtract mean and divide by standard deviation columnwise\n",
    "        numerator = X - np.mean(X, axis=0)\n",
    "        denominator = np.std(X, axis=0)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def get_covariance_matrix(self, ddof=0):\n",
    "        # calculate covariance matrix with standardized matrix A\n",
    "        C = np.dot(self.A.T, self.A) / (self.n_samples - ddof)\n",
    "        return C\n",
    "\n",
    "    def get_eigenvectors(self, C):\n",
    "        # calculate eigenvalues & eigenvectors of covariance matrix 'C'\n",
    "        self.eigenvalues, eigenvectors = np.linalg.eig(C)\n",
    "        \n",
    "        # sort eigenvalues descending and select columns based on n_components\n",
    "        n_cols = np.argsort(self.eigenvalues)[::-1][: self.n_components]\n",
    "        selected_vectors = eigenvectors[:, n_cols]\n",
    "        return selected_vectors\n",
    "\n",
    "    def project_matrix(self, eigenvectors):\n",
    "        P = np.dot(self.A, eigenvectors)\n",
    "        return P\n",
    "\n",
    "\n",
    "d = np.array([\n",
    "    [-1, .5**.5, 3], \n",
    "    [-1, 0, -2], \n",
    "    [-1, -(.5**.5), 5], \n",
    "    [1, 0, 9], \n",
    "    [1, .5**.5, 2.7], \n",
    "    [1, -(.5**.5), 0]])\n",
    "    \n",
    "my_pca = PCA(2)\n",
    "my_pca.fit_transform(d)\n",
    "print(\"Eigenvectors: \\n\", my_pca.eigenvectors)\n",
    "print(\"Eigenvalues: \\n\",my_pca.eigenvalues)\n",
    "print(\"PCA 1: \", (my_pca.eigenvalues[1]/my_pca.eigenvalues.sum())*100)\n",
    "print(\"PCA 2: \", (my_pca.eigenvalues[2]/my_pca.eigenvalues.sum())*100)\n",
    "print(\"PCA 3: \", (my_pca.eigenvalues[0]/my_pca.eigenvalues.sum())*100)\n",
    "print(\"Final Matrix: \\n\",my_pca.projected_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b03adf348b7c0ec0a1b97ce9088ce650b8dfb236a19f985122803e1df30fa8e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
